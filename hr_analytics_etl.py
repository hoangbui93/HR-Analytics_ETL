# -*- coding: utf-8 -*-
"""HR-Analytics_ETL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lbqywY9IOM5EQkKYy5XgrL-AlsKsGyCA

# **INTRODUCTION**

**Context and Content**

- A company which is active in Big Data and Data Science wants to hire data scientists among people who successfully pass some courses which conduct by the company.

- Many people signup for their training. Company wants to know which of these candidates are really wants to work for the company after training or looking for a new employment because **it helps to reduce the cost and time as well as the quality of training or planning the courses and categorization of candidates.**

- Information related to demographics, education, experience are in hands from candidates signup and enrollment.

# **DATA SOURCE**

## **1. Enrollies' data**
As enrollies are submitting their request to join the course via Google Forms, we have the Google Sheet that stores data about enrolled students, containing the following columns:
- enrollee_id: unique ID of an enrollee
- full_name: full name of an enrollee
- city: the name of an enrollie's city
- gender: gender of an enrollee

The source: https://docs.google.com/spreadsheets/d/1VCkHwBjJGRJ21asd9pxW4_0z2PWuKhbLR3gUHm-p4GI/edit?usp=sharing
"""

import pandas as pd

# Read data from Google Sheets
google_sheet_id = '1VCkHwBjJGRJ21asd9pxW4_0z2PWuKhbLR3gUHm-p4GI'
url = 'https://docs.google.com/spreadsheets/d/' + google_sheet_id + '/export?format=xlsx'
enrollies_data = pd.read_excel(url, sheet_name = 'enrollies')

enrollies_data.info()

# Handle missing values
enrollies_data['gender'] = enrollies_data['gender'].fillna('Unknown')

# Check unconsistent data
enrollies_data['gender'].unique()

enrollies_data['city'].unique()

# Convert data type
enrollies_data['full_name'] = enrollies_data['full_name'].astype(str)
enrollies_data[['city', 'gender']] = enrollies_data[['city', 'gender']].astype('category')

enrollies_data.info()

enrollies_data.head()

"""## **2. Enrollies' education**
After enrollment everyone should fill the form about their education level. This form is being digitalized manually. Educational department stores it in the Excel format here: https://assets.swisscoding.edu.vn/company_course/enrollies_education.xlsx

This table contains the following columns:
- enrollee_id: A unique identifier for each enrollee. This integer value uniquely distinguishes each participant in the dataset.
- enrolled_university: Indicates the enrollee's university enrollment status. Possible values include no_enrollment, Part time course, and Full time course.
- education_level: Represents the highest level of education attained by the enrollee. Examples include Graduate, Masters, etc.
- major_discipline: Specifies the primary field of study for the enrollee. Examples include STEM, Business Degree, etc.
"""

# Read data from Excel file
enrollies_education = pd.read_excel('https://assets.swisscoding.edu.vn/company_course/enrollies_education.xlsx', sheet_name = 'enrollies_education')

enrollies_education.info()

# Handle missing values
enrollies_education['major_discipline'] = enrollies_education['major_discipline'].fillna('Unknown')
enrollies_education['education_level'] = enrollies_education['education_level'].fillna(enrollies_education['education_level'].mode()[0])
enrollies_education['enrolled_university'] = enrollies_education['enrolled_university'].fillna(enrollies_education['enrolled_university'].mode()[0])

# Check unconsistent data
enrollies_education['enrolled_university'].unique()

enrollies_education['education_level'].unique()

enrollies_education['major_discipline'].unique()

# Convert data type
enrollies_education[['education_level', 'enrolled_university', 'major_discipline']].astype('category')

enrollies_education.info()

enrollies_education.head()

"""## **3. Enrollies' working experience**
Another survey that is being collected manually by educational department is about working experience.

Educational department stores it in the CSV format here: https://assets.swisscoding.edu.vn/company_course/work_experience.csv

This table contains the following columns:
- enrollee_id: A unique identifier for each enrollee. This integer value uniquely distinguishes each participant in the dataset.
- relevent_experience: Indicates whether the enrollee has relevant work experience related to the field they are currently studying or working in. Possible values include Has relevent experience and No relevent experience.
- experience: Represents the number of years of work experience the enrollee has. This can be a specific number or a range (e.g., >20, <1).
- company_size: Specifies the size of the company where the enrollee has worked, based on the number of employees. Examples include 50âˆ’99, 100âˆ’500, etc.
- company_type: Indicates the type of company where the enrollee has worked. Examples include Pvt Ltd, Funded Startup, etc.
- last_new_job: Represents the number of years since the enrollee's last job change. Examples include never, >4, 1, etc.
"""

# Read data from CSV file
work_experience = pd.read_csv('https://assets.swisscoding.edu.vn/company_course/work_experience.csv')

work_experience.info()

# Handle missing values
work_experience['experience'] = work_experience['experience'].fillna(work_experience['experience'].mode()[0])
work_experience['last_new_job'] = work_experience['last_new_job'].fillna(work_experience['last_new_job'].mode()[0])
work_experience[['company_size', 'company_type']] = work_experience[['company_size', 'company_type']].fillna('Unknown')

# Check unconsistent data
work_experience['experience'].unique()

work_experience['company_size'].unique()

work_experience['company_type'].unique()

work_experience['last_new_job'].unique()

# Convert data type
work_experience = work_experience.convert_dtypes()

work_experience.info()

work_experience.head()

"""## **4. Training hours**
From LMS system's database you can retrieve a number of training hours for each student that they have completed.

**Database credentials:**
- Database type: MySQL
- Host: 112.213.86.31
- Port: 3360
- Login: etl_practice
- Password: 550814
- Database name: company_course
- Table name: training_hours
"""

# Module installation and import connector
from sqlalchemy import create_engine
!pip install pymysql
import pymysql

"""From LMS system's database you can retrieve a number of training hours for each student that they have completed.

Database credentials:

- Database type: MySQL
- Host: 112.213.86.31
- Port: 3360
- Login: etl_practice
- Password: 550814
- Database name: company_course
- Table name: training_hours
"""

# Read data from SQL Database
login = 'etl_practice'
pw = '550814'
host = '112.213.86.31'
port = '3360'
db_name = 'company_course'
engine = create_engine(f'mysql+pymysql://{login}:{pw}@{host}:{port}/{db_name}')
training_hours = pd.read_sql_table('training_hours', con = engine)

training_hours.info()

training_hours.head()

"""## **5. City development index**
Another source that can be usefull is the table of City development index.

The City Development Index (CDI) is a measure designed to capture the level of development in cities. It may be significant for the resulting prediction of student's employment motivation.

It is stored here: https://sca-programming-school.github.io/city_development_index/index.html
"""

# Use pandas to read the HTML table
tables = pd.read_html('https://sca-programming-school.github.io/city_development_index/index.html')

# Assuming the first table on the webpage is the one you want (indexing starts from 0)
city_development_index = tables[0]

city_development_index.info()

city_development_index.head()

"""## **6. Employment**
From LMS database you can also retrieve the fact of employment. If student is marked as employed, it means that this student started to work in our company after finishing the course.

**Database credentials:**
- Database type: MySQL
- Host: 112.213.86.31
- Port: 3360
- Login: etl_practice
- Password: 550814
- Database name: company_course
- Table name: employment
"""

# Read data from SQL Database
login = 'etl_practice'
pw = '550814'
host = '112.213.86.31'
port = '3360'
db_name = 'company_course'
engine = create_engine('mysql+pymysql://etl_practice:550814@112.213.86.31:3360/company_course')
employment = pd.read_sql_table('employment', con = engine)

employment.info()

employment.head()

"""# **Load Data into Database**"""

# Create data warehouse in database
db_name = 'HR_DataWarehouse.db'
warehouse_engine = create_engine(f'sqlite:///{db_name}')

# Write DataFrames to database
enrollies_data.to_sql('tbl_EnrolliesData', con = warehouse_engine, if_exists = 'replace', index = False)
enrollies_education.to_sql('tbl_EnrolliesEducation', con = warehouse_engine, if_exists = 'replace', index = False)
work_experience.to_sql('tbl_WorkExperience', con = warehouse_engine, if_exists = 'replace', index = False)
training_hours.to_sql('tbl_TrainingHours', con = warehouse_engine, if_exists = 'replace', index = False)
city_development_index.to_sql('tbl_CityDevelopmentIndex', con = warehouse_engine, if_exists = 'replace', index = False)
employment.to_sql('tbl_Employment', con = warehouse_engine, if_exists = 'replace', index = False)

"""# **ðŸ’ª Advanced task**

**Refactoring**

You could notice that there is much code duplication! A lot of similar lines of code.

This is bad practice in programming sad

If you feel enough power, try to wrap repeating code into functions to make your code better structured. (in programming we name it refactoring)
"""

import pandas as pd
from sqlalchemy import create_engine

def load_google_sheet(sheet_id, sheet_name):
    url = f'https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=xlsx'
    return pd.read_excel(url, sheet_name = sheet_name)

def load_excel(url, sheet_name):
    return pd.read_excel(url, sheet_name = sheet_name)

def load_csv(url):
    return pd.read_csv(url)

def load_html_table(url):
    return pd.read_html(url)[0]

def load_table_from_mysql(table_name, user, password, host, port, database):
    uri = f'mysql+pymysql://{user}:{password}@{host}:{port}/{database}'
    engine = create_engine(uri)
    df = pd.read_sql_table(table_name, con = engine)
    return df

def save_to_sqlite(df, table_name, db_name):
    engine = create_engine(f'sqlite:///{db_name}')
    df.to_sql(table_name, con = engine, if_exists = 'replace', index = False)